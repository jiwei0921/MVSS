<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Our method learns single-image 3D reconstruction models of articulated animal categories, from just online photo collections, without any 3D geometric supervision or template shapes.">
  <meta name="keywords" content="MagicPony, Unsup3D, Unsupervised, 3D reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MagicPony: Learning Articulated 3D Animals in the Wild</title>

  <meta property="og:image" content="https://3dmagicpony.github.io/resources/og_image.jpg"/>
	<meta property="og:title" content="MagicPony: Learning Articulated 3D Animals in the Wild." />
	<meta property="og:description" content="Our method learns single-image 3D reconstruction models of articulated animal categories, from just online photo collections, without any 3D geometric supervision or template shapes." />
  <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
      if you update and want to force Twitter to re-scrape. -->
  <meta property="twitter:card"          content="summary" />
  <meta property="twitter:title"         content="MagicPony: Learning Articulated 3D Animals in the Wild." />
  <meta property="twitter:description"   content="Our method learns single-image 3D reconstruction models of articulated animal categories, from just online photo collections, without any 3D geometric supervision or template shapes." />
  <meta property="twitter:image"         content="https://3dmagicpony.github.io/resources/og_image.jpg" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VFNFH9CKNX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-VFNFH9CKNX');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./resources/carousel-horse.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <video id="banner" autoplay muted loop playsinline height="100%">
          <source src="resources/carousel.mp4"
                  type="video/mp4">
        </video>
      </div>
      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title" style="font-size: 2.2rem;">&#127904; MagicPony: Learning Articulated 3D Animals in the Wild</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://elliottwu.com/">Shangzhe Wu</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://ruiningli.com/">Ruining Li</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.robots.ox.ac.uk/~tomj/">Tomas Jakab</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://chrirupp.github.io/">Christian Rupprecht</a>,
            </span>
            <span class="author-block">
              <a href="https://www.robots.ox.ac.uk/~vedaldi/">Andrea Vedaldi</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Visual Geometry Group, University of Oxford</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">(* equal contribution)</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">CVPR 2023</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2211.12497"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="resources/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle">
        Our method learns single-image 3D reconstruction models of articulated animal categories, from just online photo collections, without any 3D geometric supervision or template shapes.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We consider the problem of learning a function that can estimate the 3D shape, articulation, viewpoint, texture, and lighting of an articulated animal like a horse, given a single test image. We present a new method, dubbed <b>MagicPony</b>, that learns this function purely from in-the-wild single-view images of the object category, with minimal assumptions about the topology of deformation. At its core is an implicit-explicit representation of articulated shape and appearance, combining the strengths of neural fields and meshes. In order to help the model understand an object's shape and pose, we distil the knowledge captured by an off-the-shelf self-supervised vision transformer and fuse it into the 3D model. To overcome common local optima in viewpoint estimation, we further introduce a new viewpoint sampling scheme that comes at no added training cost. Compared to prior works, we show significant quantitative and qualitative improvements on this challenging task. The model also demonstrates excellent generalisation in reconstructing abstract drawings and artefacts, despite the fact that it is only trained on real images.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <video id="video" playsinline height="100%" controls>
          <source src="https://www.robots.ox.ac.uk/~szwu/storage/23_magicpony/video.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <!--/ Paper video. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Single-Image 3D Reconstruction</h2>

        <h2 class="title is-5">Real Horse Images and Realistic Paintings</h2>
        <div class="content has-text-justified">
          <p>
            After training, given a single image of a new instance, the model reconstructs articulated 3D shape and appearance of it, which can be animated and re-rendered from arbitrary viewpoints.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/horse_real.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Generalisation to Abstract Horse Drawings and Artefacts</h2>
        <div class="content has-text-justified">
          <p>
            The model also generalises to abstract drawings and artefacts, despite being trained only on real images.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/horse_abstract.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Other Animal Categories: Giraffes, Zebras and Cows</h2>
        <div class="content has-text-justified">
          <p>
            After finetuning, our model also generalises to various animal categories with highly different underlying shapes.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/other_animals.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Frame-by-Frame Reconstruction on Videos</h2>
        <div class="content has-text-justified">
          <p>
            We run the model on videos frame by frame, and obtain temporally consistent reconstructions.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline style="max-width: 86%; display: block; margin: auto;">
            <source src="resources/video_recon.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{wu2023magicpony,
  author    = {Shangzhe Wu and Ruining Li and Tomas Jakab and Christian Rupprecht and Andrea Vedaldi},
  title     = {{MagicPony}: Learning Articulated 3D Animals in the Wild},
  journal   = {CVPR},
  year      = {2023}
}</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    <p>
      We would like to thank the authors of <a href="https://github.com/NVlabs/nvdiffrec">nvdiffrec</a> for open-sourcing the code for DMTet and rendering. We are also grateful to Tengda Han, Shu Ishida, Dylan Campbell, Eldar Insafutdinov, Luke Melas-Kyriazi, Ragav Sachdeva and Sagar Vaze for insightful discussions, and Guanqi Zhan and Jaesung Huh for proofreading.
    </p>
    <p>
      Shangzhe Wu is supported by Meta Research. Tomas Jakab is supported by ERC-CoG UNION 101001212. Christian Rupprecht is supported by VisualAI EP/T028572/1 and ERC-CoG UNION 101001212. Andrea Vedaldi is supported by ERC-CoG UNION 101001212.
    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage template is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
